---
title: "Lecture 11: Logistic Regression for Prediction"
author: "Albert Y. Kim"
date: "October 7, 2016"
output: ioslides_presentation
---

<style>
h2 { 
 color: #3399ff;		
}
h3 { 
 color: #3399ff;		
}
slides > slide.backdrop {
  background: white;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(tidyr)
```



## Regression to Two Ends:

* For explanation:
* For prediction:





## Binary Outcome Variables

Let out outcomes be **binary**.  

i.e. for $i=1, \ldots, n$ observations

* $y_i = 1$ if some condition holds
* $y_i = 0$ if some condition does not hold

We're interested in $p_i = \mbox{Pr}(y_i = 1)$.



## Logistic Regression

Logistic regression is preferred over linear regression here because you might end up with fitted probabilities $\widehat{p}_i = \widehat{\mbox{Pr}}(y_i = 1)$ that are either

* less than 0
* greater than 1

So we use the not the first model, but the second:

$$
\begin{eqnarray}
p_i &=& \beta_1 X_{i1} + \ldots  + \beta_k X_{ik}\\
\mbox{logit}(p_i)=\log\left(\frac{p_i}{1-p_i}\right) &=& \beta_1 X_{i1} + \ldots  + \beta_k X_{ik}
\end{eqnarray}
$$







## Questions

* Knowing nothing about a user, what is your best guess of the probability that the user is female?
* Is height predictive of a user's sex?




## Acknowledgements

Thanks to Christian Rudder from OkCupid and OkTrends for agreeing to the data's use.

Journal of Statistics Education paper can be found [here](https://github.com/rudeboybert/JSE_OkCupid).
