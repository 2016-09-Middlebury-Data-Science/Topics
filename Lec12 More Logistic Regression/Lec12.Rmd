---
title: "Lecture 12: More Logistic Regression"
author: "Albert Y. Kim"
date: "October 10, 2016"
output: ioslides_presentation
---

<style>
h2 { 
 color: #3399ff;		
}
h3 { 
 color: #3399ff;		
}
slides > slide.backdrop {
  background: white;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(tidyr)
```





## Recall: Binary Outcome Variables

We model the relationship between the

* observed binary outcome variable $y$
* predictor variables $x_1, \ldots, x_k$

as the following where $p = \mbox{Pr}(y = 1)$:

$$
\begin{eqnarray}
\mbox{logit}(p)=\log\left(\frac{p}{1-p}\right) &=& \beta_0 + \beta_1 x_{1} + \ldots  + \beta_k x_{k}
\end{eqnarray}
$$





## Recall: Solving for $p$

If the **linear regression equation** is this:

$$
\begin{eqnarray}
\mbox{logit}(p)=\log\left(\frac{p}{1-p}\right) &=& \beta_0 + \beta_1 x_{1} + \ldots  + \beta_k x_{k}
\end{eqnarray}
$$

then to solve for $p$, we use:

$$
\begin{eqnarray}
p &=& \frac{1}{1+\exp\left(-(\beta_0 + \beta_1 x_{1} + \ldots  + \beta_k x_{k})\right)} 
\end{eqnarray}
$$




## Logistic Regression in R

> * Use `MODEL_NAME <- glm(OUTCOME_VAR ~ PREDICTOR_1 + PREDICTOR2, data=DATASET, family="binomial")`
> * For an intercept only model i.e. no predictors, use `glm(OUTCOME_VAR ~ 1, data=DATASET, family="binomial")`
> * Categorical variable: First level of factor is baseline (exercise)



## Fitted Values

After we fit a regression and obtain the fitted $\widehat{\beta}_0, \ldots, \widehat{\beta}_k$, we can compute the fitted probabilities $\widehat{p}$:

$$
\begin{eqnarray}
\widehat{p} &=& \frac{1}{1+\exp\left(-(\widehat{\beta}_0 + \widehat{\beta}_1 x_{1} + \ldots  + \widehat{\beta}_k x_{k})\right)} 
\end{eqnarray}
$$

In R all $n$ fitted values can be obtained via: `fitted(MODEL_NAME)`





## Two Branches of Machine Learning

> * **Unsupervised learning**: no outcome variable of interest. Examples include clustering, where you group observations based on similar traits.
> * **Supervised learning**: there is an outcome varible of interest. Examples include regression and decision trees
> * In our case: sex is the outcome variable which we use as the **ground truth** to compare our predictions to.



## Exercise for Today

> 1. We fit a logistic regression model using the outcome and predictor variables $y$ and $x_1, \ldots, x_k$
> 1. We obtain a prediction $\widehat{{\tt{is.female}}}$ for each user using $\widehat{p}$
> 1. We compare our prediction $\widehat{{\tt{is.female}}}$ to the truth ${\tt{is.female}}$ and compute **error** rates



## Conceptual Leap: Model Training vs Testing

> * Here we know the true gender of each user, and thus we can evaluate if we were right or wrong. i.e. error rates.
> * But if we know each user's true sex, then why are we predicting it?
> * This is called **model training**: we need to train the model using data that includes the true sex
> * After we've trained the model, we **test** the model on observations where we **don't know the sex**.






